
paths:
  checkpoint_dir: checkpoints   # Directory to store model checkpoints and tensorboard, will be created if not existing.
  data_dir: datasets            # Directory to store processed data, will be created if not existing.

preprocessing:
  languages: ['hsb', 'dsb']    # All languages in the dataset.

  # Text (grapheme) and phoneme symbols, either provide a string or list of strings.
  # Symbols in the dataset will be filtered according to these lists!
  text_symbols: "abcdefghijklmnopqrstuvwxyzßäöüčćěłńóřŕśšžźáéíóúýĺ"
  # text_symbols: 'ížúctdpéxhwazřqfvńňęäimsßjŕgćyár­łźśšukoóàýbżněölüeč' [consonants with an accent are dsb only]
  phoneme_symbols: ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'ß', 'ä', 'ö', 'ü', 'č', 'ć', 'ě', 'ł', 'ń', 'ó', 'ř', 'ŕ', 'ś', 'š', 'ž', 'ź', 'í', 'ú', 'é', 'ĺ', 'á', 'ó', 'ý']
  # phoneme_symbols: ['í', 'ž', 'ú', 'c', 't', 'd', 'p', 'é', 'x', 'h', 'w', 'ĺ', 'a', 'z', 'ď', 'ť', 'ř', 'q', 'f', 'v', 'ń', '7', 'ň', 'ę', 'ä', 'i', 'm', 's', 'ß', 'j', '_', 'ŕ', 'g', 'ć', 'y', '{', '²', '8', 'á', 'r', 'ł', 'ź', '#', '©', 'ś', '=', 'š', '%', '3', 'u', '«', '—', 'k', 'o', 'ó', 'à', ' ', 'ý', 'b', 'ż', 'n', '$', '1', '>', 'ě', 'ö', '﷽', 'l', '/', '*', '„', 'ü', '½', '9', '0', '2', 'e', '§', '!', 'č', '[']

  char_repeats: 3                # Number of grapheme character repeats to allow for mapping to longer phoneme sequences.
                                 # Set to 1 for autoreg_transformer.
  lowercase: true                # Whether to lowercase the grapheme input.
  n_val: 5000                    # Default number of validation data points if no explicit validation data is provided.

model:
  type: 'transformer'            # Whether to use a forward transformer or autoregressive transformer model.
                                 # Choices: ['transformer', 'autoreg_transformer']
  d_model: 512
  d_fft: 1024
  layers: 6
  dropout: 0.1
  heads: 4

training:

  # Hyperparams for learning rate and scheduler.
  # The scheduler is reducing the lr on plateau of phoneme error rate (tested every n_generate_steps).

  learning_rate: 0.0001              # Learning rate of Adam.
  warmup_steps: 100                # Linear increase of the lr from zero to the given lr within the given number of steps.
  scheduler_plateau_factor: 0.5      # Factor to multiply learning rate on plateau.
  scheduler_plateau_patience: 10     # Number of text generations with no improvement to tolerate.
  batch_size: 32                     # Training batch size.
  batch_size_val: 32                 # Validation batch size.
  epochs: 10                        # Number of epochs to train.
  generate_steps: 500              # Interval of training steps to generate sample outputs. Also, at this step the phoneme and word
                                     # error rates are calculated for the scheduler.
  validate_steps: 500              # Interval of training steps to validate the model
                                     # (for the autoregressive model this is teacher-forced).
  checkpoint_steps: 100000           # Interval of training steps to save the model.
  n_generate_samples: 10             # Number of result samples to show on tensorboard.
  store_phoneme_dict_in_model: true  # Whether to store the raw phoneme dict in the model.
                                     # It will be loaded by the phonemizer object.
